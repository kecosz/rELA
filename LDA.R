#install.packages(c("topicmodels", "Matrix", "glmnet", "quanteda"))
#library("topicmodels")
#library("Matrix")
#library("quanteda")
#library("stats")

#'preprocessing_LDA
#'@description Processing abundance data matrix for LDA
#'
#'@param X : taxa abundance data (without binarization)
#'@param M: multiplied factors for converting to interger matrix
#'@param train_idx : indices used for training.
#'@param Normalize : (bool) If true, the data will be normalized so that the sum of each row is 1.
#'
#'@returns data : processed abundance table
#'@returns train_idx : indices used for training
#'----
preprocessing_LDA <- function(X, train_idx = NULL, Normalize=TRUE, M = 1000) {
  stopifnot(is.matrix(X) || is.data.frame(X))
  X <- as.matrix(X)
  if (any(X < 0)) stop("Input table have negative values. The data must contain 0 or positive values.")
  if (is.null(colnames(X))) colnames(X) <- paste0("taxon_", seq_len(ncol(X)))
  if (is.null(rownames(X))) rownames(X) <- paste0("sample_", seq_len(nrow(X)))
  if (is.null(train_idx)) train_idx <- seq_len(nrow(X))  # All data are used if train_idx is not specified.
  
  # ---- float values are converted to integer（LDA） ----
  row_sums <- rowSums(X)
  # Rows which contain only 0 values are eliminated. 
  keep <- row_sums > 0
  if (!all(keep)) {
    warning(sprintf("Samples containing only 0 (%d rows) are eliminated", sum(!keep)))
    X <- X[keep, , drop = FALSE]
    row_sums <- rowSums(X)
    # mapping train_idx
    train_idx <- which(keep)[which(keep) %in% train_idx]
  }
  if(Normalize){
    P <- sweep(X, 1, row_sums, "/") # Normalize each row
  }else{P <- X}
  C <- round(P * M)                                     # Convert to integer by multiplying M
  C[C < 0] <- 0
  list(data=C,train_idx=train_idx)
}

#'make_stm
#'@description create stm object from count matrix
make_stm <- function(count_mat) {
  i_idx <- as.integer(row(count_mat))
  j_idx <- as.integer(col(count_mat))
  v <- as.numeric(count_mat)
  nz <- v > 0
  slam::simple_triplet_matrix(i = i_idx[nz],
                              j = j_idx[nz],
                              v = v[nz],
                              nrow = nrow(count_mat),
                              ncol = ncol(count_mat),
                              dimnames = list(rownames(count_mat), colnames(count_mat)))
}

#'select_k
#'@description Select the number of optimal assemblages(K): Comparison by log-likelihood/perplexitye.
#'
#'@param dtm : simple triplet matrix generated by dtm
#'@param K_grid : array of Ks used for searching optimal clusters (K)
#'@param seed : the random seed for clustering.
#'----
select_k <- function(dtm, K_grid = c(2,3,4,5,6,8,10), seed = 1234) {
  set.seed(seed)
  fits <- lapply(K_grid, function(K) {
    fit <- LDA(dtm, k = K, method = "VEM",
               control = list(seed = seed, estimate.alpha = TRUE, verbose = 0))
    ll <- logLik(fit)
    data.frame(K = K, logLik = as.numeric(ll))
  })
  sel <- do.call(rbind, fits)
  sel$perplexity <- exp(-sel$logLik / sum(dtm$v))  # 近似
  sel[order(sel$perplexity), ]
}

#'fit_lda
#'@description LDA fitting using specified number of clusters (K).
fit_lda <- function(dtm, K, seed = 1234) {
  set.seed(seed)
  LDA(dtm, k = K, method = "VEM",
      control = list(seed = seed, estimate.alpha = TRUE, verbose = 0))
}


#'extract_assemblages
#'@description LDA fitting using specified number of clusters (K).
#'@return theta: sample-assemblage matrix
#'@return beta: assemblage-taxa matrix)
extract_assemblages <- function(fit) {
  theta <- posterior(fit)$topics    # doc-topic (samples x K)
  beta  <- posterior(fit)$terms     # topic-term (K x taxa)
  list(theta = theta, beta = beta)
}

#'lda_for_ela
#'@description Run LDA using optimal assemblage numbers (K)
#'
#'@param abtable : taxa abundance data (without binarization)
#'@param M: multiplied factors for converting to interger matrix
#'@param K_grid : array of Ks used for searching optimal clusters (K)
#'@param K_fixed: (integer) used for stm.if NULL, optimal K will be searched using K_grid.
#'@param seed : the random seed for clustering.
#'@param train_idx : indices used for training.
#'@param Normalize : (bool) If true, the data will be normalized so that the sum of each row is 1.
#'@returns k_opt : used K for lda
#'@returns theta : sample-assemblage matrix
#'@returns beta : assemblage-taxa matrix)
#'----
lda_for_ela <- function(abtable, M=1000,K_grid = 3:8,
                        K_fixed = NULL,seed = 1234,train_idx = NULL,
                        Normalize=TRUE) {
  
  # pre-processing
  count_mat <- preprocessing_LDA(abtable,M=M,train_idx = train_idx,
                                 Normalize = Normalize)
  dtm <- make_stm(count_mat$data)
  
  # seearch optimal K
  if (is.null(K_fixed)) {
    k_stats <- select_k(dtm, K_grid = K_grid, seed = seed) 
    k_opt <- k_stats[1,1]
    message(sprintf("Selected K = %d (perplexity-min rule)", k_opt))
  } else {
    k_opt <- K_fixed
  }
  
  # Run LDA
  lda_fit <- fit_lda(dtm, k_opt, seed = seed) 
  print("LDA finished.")
  LDA_result <- extract_assemblages(lda_fit)
  lda_mat <- LDA_result[[1]]
  assemblages <- LDA_result[[2]] 
  lda_mat[lda_mat <= 1e-10] <- 0
  assemblages[assemblages <= 1e-10] <- 0
  list(k_summary = k_stats,
       theta = lda_mat,   # samples x K
       beta  = assemblages   # K x taxa
       )
}

#'RandDirichletSet
#'@description Create random dataset using Dirichlet-Multinominal distribution
#'
#'@param n_species : the number of species
#'@param n_basins : the number of basins (clusters)
#'@param seed : the number of samples 
#'@param n_reads : the number of reads in each sample.
#'
#'@returns data : abndance matrix based on the DM distribution 
#'@returns prob_vectors : probability vector of dirichlet distribution
#'@returns vector_index : indices (indicating category number which each sample belongs to)
#'----

RandDirichletSet <- function(n_species, n_basins, n_samples, n_reads=10000) {
  # Step 1: Generate probability vectors based on Dirichlet distributions 
  alpha_dirichlet <- rep(1, n_species) # length:nspecies
  prob_vectors <- lapply(1:n_basins, function(x) rdirichlet(1, alpha_dirichlet))  # sampling Dirichlet
  
  # Step 2: Determine how many samples belong to each cluster.
  alpha_weights <- rep(1, n_basins)  
  order_weights <- rdirichlet(1, alpha_weights)  # sampling probability
  order_counts <- rmultinom(1, n_samples, order_weights)  # Multinominal distributions
  
  # Step 3: Generate simulated read count data with size of n_samples.
  final_data <- list()
  
  for (idx in 1:n_basins) {
    count <- order_counts[idx]
    for (i in 1:count) {
      vec <- rmultinom(1, n_reads, prob_vectors[[idx]])  # Dirichlet sampling
      final_data <- append(final_data, list(vec))
    }
  }
  
  # Step 5: Summarizing into a data.frame
  final_data_df <- t(do.call(cbind, final_data))  # convert to dataframe
  print(dim(final_data_df))
  colnames(final_data_df) <- paste("species", 1:n_species, sep = ".")  
  rownames(final_data_df) <- paste("sample", 1:n_samples, sep = ".") 
  # Add cluster index
  vector_index <- rep(1:n_basins, order_counts)
  
  return(list(data = final_data_df, prob_vectors = prob_vectors, vector_index = vector_index))
}
